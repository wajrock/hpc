# Introduction au Calcul Haute Performance sur GPU

## 1\. Introduction : Du Calcul S√©quentiel au Parall√©lisme 

Pendant des d√©cennies, l'informatique a repos√© sur un seul ma√Ætre √† bord : le **CPU** (Processeur Central). Imaginez-le comme un gestionnaire ultra-rapide, capable de passer d'un email √† un fichier Excel puis √† une page web en un √©clair. Sa force, c'est la **logique s√©quentielle** : il traite les probl√®mes complexes les uns apr√®s les autres.

Cependant, l'arriv√©e des jeux vid√©o 3D, du traitement d'image haute d√©finition et plus r√©cemment de l'Intelligence Artificielle (*Deep Learning*) a chang√© la donne. Ici, il ne s'agit plus de r√©soudre une √©quation complexe, mais de traiter **des millions de petites donn√©es identiques simultan√©ment** (par exemple : changer la couleur de 8 millions de pixels sur un √©cran 4K).

Face √† cette charge, le CPU s'essouffle. Ce cours vous introduit au **calcul GPU** (Graphics Processing Unit). C'est une approche qui abandonne la "vitesse pure sur une t√¢che" au profit du **parall√©lisme massif** : faire moins vite individuellement, mais faire tout en m√™me temps.


### 1.1 Comparaison Architecturale : Latence versus D√©bit

La diff√©rence fondamentale entre CPU et GPU r√©side dans la mani√®re dont les puces investissent leurs ressources (transistors) :

  * **CPU (Optimis√© pour la Latence) :** Con√ßu pour que *chaque instruction* se termine le plus vite possible. Il dispose de peu de c≈ìurs, mais chacun est puissant, dot√© de larges caches et d'unit√©s de pr√©diction complexes pour g√©rer la logique s√©quentielle.

  * **GPU (Optimis√© pour le D√©bit) :** Sacrifie la complexit√© individuelle pour maximiser le nombre d'unit√©s de calcul (ALU). Il ne cherche pas √† aller vite pour *une* t√¢che, mais √† en ex√©cuter des milliers simultan√©ment.

> **üí° L'Analogie : Le Professeur et la Classe**
>
>   * **Le CPU est un Professeur de Math√©matiques √©m√©rite (ex: Einstein).** Il est brillant et rapide. Il peut r√©soudre des int√©grales complexes en un clin d'≈ìil. Mais s'il doit corriger 10 000 copies d'addition simple, il devra les faire l'une apr√®s l'autre. Cela prendra des heures.
>   * **Le GPU est une classe de 1 000 √©l√®ves de primaire.** Individuellement, ils sont lents et ne savent faire que des op√©rations simples. Mais si vous distribuez les 10 000 copies, ils peuvent en corriger 1 000 √† la fois. Le travail est fini en quelques secondes.

### 1.2 Traduction en CUDA

En programmation CUDA, nous √©crivons tout dans le m√™me fichier (extension `.cu`), mais il faut comprendre que deux mondes physiquement s√©par√©s cohabitent :

1.  **Le Host (H√¥te) :** C'est votre **CPU**. Il joue le r√¥le de **Chef d'Orchestre**. Il ne fait pas le calcul intensif lui-m√™me, mais il g√®re la logistique : il pr√©pare les donn√©es, les envoie au GPU et donne le signal de d√©part.
2.  **Le Device (P√©riph√©rique) :** C'est votre **GPU**. C'est l'**Usine**. Il attend les ordres et les donn√©es pour lancer ses milliers d'ouvriers.

> **üí° L'Analogie : Les Consignes vs L'Exercice**
>
> * **Le Code Host (CPU)** correspond aux **consignes orales** du professeur : *"Prenez une feuille, recopiez l'exercice au tableau, vous avez 1 heure."*
> * **Le Code Device (GPU)** correspond √† **l'√©nonc√© de l'exercice** lui-m√™me : *"Calculez la racine carr√©e de x."* Chaque √©l√®ve (Thread) va appliquer cet √©nonc√© √† sa propre feuille.

Pour distinguer ces deux mondes dans le code, CUDA utilise un mot-cl√© sp√©cial : `__global__`.

```cpp
// Code CPU (Le Professeur)
void main() {
    // Pr√©pare les donn√©es et lance l'ordre √† la classe
    monKernel<<<...>>>(...); 
}

// Code GPU (Les √âl√®ves)
// Le mot-cl√© "__global__" indique que cette fonction est ex√©cut√©e sur le GPU
__global__ void monKernel(float* data) {
    // Instruction simple ex√©cut√©e par des milliers de threads
}
```

## 2\. Architecture Mat√©rielle et Mod√®le d'Ex√©cution

Pour exploiter toute la puissance du GPU, CUDA d√©compose l'ex√©cution en une hi√©rarchie stricte. Une mauvaise gestion de cette structure peut fortement impacter les performances.

### 2.1 Hi√©rarchie Logique : Grille, Blocs et Threads

CUDA organise les "ouvriers" en trois niveaux :

1.  **La Grille (Grid) :** L'ensemble du probl√®me √† r√©soudre (ex: une image enti√®re).
2.  **Le Bloc (Thread Block) :** Un sous-groupe de la grille. Les threads d'un m√™me bloc peuvent communiquer via une m√©moire partag√©e rapide.
3.  **Le Thread (Fil d'ex√©cution) :** L'unit√© fondamentale qui traite un seul point de donn√©e (ex: un pixel).

> **üí° L'Analogie : L'Organisation de l'√âcole**
>
>   * **La Grille** est l'√©cole enti√®re mobilis√©e pour un examen.
>   * **Le Bloc** est une salle de classe sp√©cifique. Les √©l√®ves d'une m√™me salle peuvent se parler (m√©moire partag√©e), mais ne peuvent pas copier sur les √©l√®ves de la salle voisine.
>   * **Le Thread** est un √©l√®ve unique assis √† son bureau.

### 2.2 Traduction en CUDA : Le Calcul de Coordonn√©es

Chaque thread doit savoir "qui il est" pour savoir "quelle donn√©e traiter". Il calcule ses coordonn√©es uniques ($x, y$) √† partir de son index dans le bloc et de la position du bloc dans la grille.

```cpp
__global__ void imageKernel(int* image, int width) {
    // Qui suis-je ? (Calcul de l'index global)
    // blockIdx.x : Num√©ro de ma salle de classe
    // blockDim.x : Nombre d'√©l√®ves par salle
    // threadIdx.x : Mon num√©ro de place dans la salle
    
    int x = blockIdx.x * blockDim.x + threadIdx.x;
    int y = blockIdx.y * blockDim.y + threadIdx.y;

    // Je travaille uniquement si je suis dans les limites de l'image
    if (x < width && y < height) {
        // Traitement de MON pixel unique
    }
}
```

### 2.3 L'Unit√© R√©elle : Le Warp et la Divergence

Physiquement, le GPU n'ex√©cute pas les threads un par un, mais par groupes de 32 appel√©s **Warps**. Ils suivent le mod√®le SIMT (*Single Instruction, Multiple Threads*) : ils doivent tous ex√©cuter la m√™me instruction au m√™me moment.

**Le Pi√®ge : La Divergence**
Si vous mettez un `if-else` dans votre code, et que la moiti√© du Warp va dans le `if` et l'autre dans le `else`, le GPU doit ex√©cuter les deux branches s√©quentiellement, divisant la performance par deux.

> **üí° L'Analogie : La Dict√©e**
> Le professeur (l'unit√© de contr√¥le) dicte √† une rang√©e d'√©l√®ves (un Warp).
>
>   * *Cas id√©al :* "√âcrivez tous le mot 'Chat'". Tous √©crivent en m√™me temps.
>   * *Divergence :* "Si vous avez un stylo bleu, √©crivez 'Chat', sinon √©crivez 'Chien'". Le prof doit d'abord faire √©crire ceux au stylo bleu (les autres attendent), puis ceux au stylo rouge. On perd du temps.

-----

## 3\. Gestion de la M√©moire : Le Nerf de la Guerre

Dans une application GPU, la puissance de calcul brute est rarement le facteur limitant. Le v√©ritable goulot d'√©tranglement est la gestion de la m√©moire. Une application mal optimis√©e peut passer l'essentiel de son temps √† attendre l'arriv√©e des donn√©es.

### 3.1 Le Goulot d'√âtranglement : le Bus PCIe

L'architecture repose sur la s√©paration physique de deux espaces m√©moires (Host RAM et Device VRAM), connect√©s par le bus PCI Express (PCIe). La bande passante du bus PCIe est consid√©rablement plus lente (ex: 16-32 Go/s pour une liaison x16) que la bande passante interne de la m√©moire du GPU (ex: 900 Go/s pour une architecture Volta). Cette disparit√© cr√©e le goulot d'√©tranglement PCIe qui doit √™tre contourn√©.

La strat√©gie d'optimisation fondamentale consiste √† minimiser les op√©rations de Transfert Host $\leftrightarrow$ Device : il est imp√©ratif d'envoyer la totalit√© des donn√©es d'entr√©e au d√©but, d'ex√©cuter le calcul intensif sur le Device, puis de ne rapatrier que le r√©sultat final 4

> **üí° L'Analogie : La Biblioth√®que et la Salle de Classe**
>* **La RAM CPU (Host)** est la Biblioth√®que Universitaire (Source).
>* **La VRAM GPU (Device)** est la Salle d'Examen (Travail).
>* **Le Bus PCIe** est la Camionnette de Livraison.
>
>Le co√ªt majeur de l'op√©ration est li√© √† la latence de chaque transfert (le temps d'attente pour charger et d√©charger la camionnette). Pour maximiser l'efficacit√© (d√©bit), il ne faut jamais envoyer une camionnette pour un seul livre (petit transfert). Il faut consolider les besoins en remplissant la camionnette au maximum de sa capacit√© avec toutes les donn√©es requises, et n'effectuer qu'un seul aller-retour entre l'H√¥te et le Device.

### 3.2 Traduction en CUDA : Allocation et Transfert

La gestion m√©moire ressemble au C standard (`malloc`, `memcpy`) mais avec le pr√©fixe `cuda`.

```cpp
void gestionMemoire(int imageSize) {
    unsigned char *h_image, *d_image;

    // 1. Allocation CPU (Host)
    h_image = (unsigned char*)malloc(imageSize);

    // 2. Allocation GPU (Device) - On pr√©pare le tableau noir
    cudaMalloc((void**)&d_image, imageSize);

    // 3. Transfert CPU -> GPU (La camionnette part)
    cudaMemcpy(d_image, h_image, imageSize, cudaMemcpyHostToDevice);

    // ... Ex√©cution du Kernel sur d_image ...

    // 4. R√©cup√©ration GPU -> CPU (La camionnette revient)
    cudaMemcpy(h_image, d_image, imageSize, cudaMemcpyDeviceToHost);
    
    cudaFree(d_image);
}
```


Voici la restructuration compl√®te de la section 4, int√©grant les r√©sultats r√©els du benchmark (qui sont excellents \!), les analogies demand√©es et une pr√©sentation acad√©mique soign√©e.

-----

## 4\. Application Pratique : Impl√©mentation Parall√®le d'un Filtre S√©pia

Cette section constitue l'√©tude de cas o√π tous les concepts architecturaux et de gestion m√©moire sont appliqu√©s. Nous utilisons l'application d'un filtre S√©pia, un algorithme parfaitement adapt√© au GPU car intrins√®quement massivement parall√®le.

### 4.1 Justification du Choix d'Algorithme

Le traitement d'image est un cas id√©al car chaque pixel est ind√©pendant (*embarrassingly parallel*). L'algorithme S√©pia consiste √† appliquer une transformation matricielle √† chaque pixel RGB pour obtenir un effet "vieille photo".

**Strat√©gie de Projection :** Nous appliquons une strat√©gie de mappage **un √† un** (1:1) : **Un Thread CUDA est responsable du traitement d'Un seul Pixel de l'image.**

### 4.2 D√©fi Technique : Le Mapping 2D vers 1D

Bien qu'une image soit une grille 2D (lignes et colonnes), la m√©moire vid√©o (VRAM) la stocke comme un tableau lin√©aire continu (1D). Chaque thread doit donc calculer son adresse unique dans ce "ruban" m√©moire.

> **üí° L'Analogie : La Biblioth√®que**
> Imaginez une biblioth√®que avec 10 √©tag√®res ($y$) de 100 livres ($x$) chacune.
> Si vous voulez le 5√®me livre de la 3√®me √©tag√®re, combien de livres y a-t-il avant lui ?
>
>   * Vous devez passer les 2 √©tag√®res compl√®tes pr√©c√©dentes ($y \times \text{largeur}$).
>   * Plus les 5 livres de l'√©tag√®re actuelle ($+ x$).
>
> **Formule :** $\text{Index} = (y \times \text{Largeur}) + x$

### 4.3 Le Noyau CUDA et l'Optimisation d'Acc√®s

Le code du noyau (`sepiaKernel`) ne contient aucune boucle `for`. Il d√©crit l'action d'un seul thread sur un seul pixel.

#### 4.3.1 Le Code du Kernel (Device)

```cpp
__global__ void sepiaKernel(unsigned char* image, int width, int height) {
    // 1. Calcul des coordonn√©es globales 2D (L'√©l√®ve trouve sa place)
    int x = blockIdx.x * blockDim.x + threadIdx.x;
    int y = blockIdx.y * blockDim.y + threadIdx.y;

    // 2. V√©rification des limites (Guard)
    if (x < width && y < height) {
        
        // 3. Conversion 2D -> 1D (Mapping m√©moire)
        // Multipli√© par 3 car chaque pixel a 3 composantes (R, G, B)
        int tid = (y * width + x) * 3;

        // Lecture (Acc√®s coalesc√© optimis√©)
        float r = image[tid];
        float g = image[tid+1];
        float b = image[tid+2];

        // Calcul S√©pia (Op√©ration arithm√©tique SIMT)
        float new_r = (r * 0.393f) + (g * 0.769f) + (b * 0.189f);
        float new_g = (r * 0.349f) + (g * 0.686f) + (b * 0.168f);
        float new_b = (r * 0.272f) + (g * 0.534f) + (b * 0.131f);

        // √âcriture (Saturation √† 255 pour √©viter les d√©bordements visuels)
        image[tid]   = (unsigned char)fminf(255.0f, new_r);
        image[tid+1] = (unsigned char)fminf(255.0f, new_g);
        image[tid+2] = (unsigned char)fminf(255.0f, new_b);
    }
}
```

#### 4.3.2 Configuration et Lancement (Host)

Le CPU doit d√©finir la taille de la grille (combien de blocs ?) pour couvrir toute l'image.

Pour mieux comprendre imaginez que vous devez transporter 100 √©l√®ves (pixels) et vos bus (blocs) ont 16 places. $100 / 16 = 6.25$. Si vous prenez 6 bus, 4 √©l√®ves ne pourrons pas monter. Il faut donc commander **7 bus** (arrondi sup√©rieur), m√™me si le dernier part partiellement vide.

```cpp
// Configuration standard : Blocs carr√©s de 16x16 threads
dim3 threadsPerBlock(16, 16); 

// Calcul du nombre de blocs (Arrondi sup√©rieur)
dim3 numBlocks((width + 15) / 16, (height + 15) / 16);

// Lancement du Kernel
sepiaKernel<<<numBlocks, threadsPerBlock>>>(d_image, width, height);
cudaDeviceSynchronize(); // Attente de la fin du calcul
```

### 4.4 Analyse de Performance R√©elle

Pour valider l'approche, nous avons effectu√© un test de charge ("Stress Test") sur une image 4K ($3840 \times 2160$) trait√©e 100 fois cons√©cutivement.

**R√©sultats Exp√©rimentaux (Google Collab T4 GPU) :**

| M√©trique | CPU (S√©quentiel) | GPU (Parall√®le) |
| :--- | :--- | :--- |
| **Temps Total** | 8 125 ms | 49 ms |
| **Temps par Image** | \~81 ms | \~0.5 ms |
| **D√©bit** | \~0.1 Gigapixels/s | \~16.9 Gigapixels/s |

**Facteur d'Acc√©l√©ration (Speedup) : $\times 165.4$**

**Interpr√©tation :**
Le GPU traite l'image **165 fois plus vite** que le CPU. L√† o√π le CPU traite les pixels un par un s√©quentiellement (latence cumulative), le GPU lance 8.3 millions de threads simultan√©ment.

### 4.5. Conclusion 

En r√©sum√©, il ne faut pas retenir que le GPU est "plus rapide" que le CPU (sa fr√©quence en MHz est souvent inf√©rieure), mais qu'il est **massivement parall√®le**.

Ce cours vous a invit√© √† un changement de philosophie fondamental : nous sommes pass√©s d'une architecture optimis√©e pour la **latence** (ex√©cuter une t√¢che le plus vite possible) √† une architecture d√©di√©e au **d√©bit** (ex√©cuter des milliers de t√¢ches simultan√©ment).

Ma√Ætriser CUDA, ce n'est pas seulement apprendre une nouvelle syntaxe. C'est comprendre comment transformer un probl√®me temporel (attendre la fin d'une boucle) en un probl√®me spatial (occuper toute la surface de la puce avec des milliers de threads). C'est cette capacit√© √† "diviser pour r√©gner" √† grande √©chelle qui rend aujourd'hui possibles les avanc√©es majeures en *Deep Learning* et en simulation scientifique.

