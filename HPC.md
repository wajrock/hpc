# Introduction au Calcul Haute Performance sur GPU

## 1\. Introduction : Du Calcul S√©quentiel au Parall√©lisme 

L'informatique traditionnelle repose sur le processeur central (CPU), con√ßu pour traiter des s√©ries d'instructions h√©t√©rog√®nes avec une rapidit√© d'ex√©cution (latence) minimale. Cependant, l'av√®nement du *Deep Learning* et du traitement d'image a impos√© des charges de travail diff√©rentes : le traitement simultan√© de gigaoctets de donn√©es.

Ce cours introduit le **calcul GPU** (Graphics Processing Unit), une approche qui d√©laisse la vitesse pure d'ex√©cution unitaire au profit d'un parall√©lisme massif.

### 1.1 Comparaison Architecturale : Latence versus D√©bit

La diff√©rence fondamentale entre CPU et GPU r√©side dans la mani√®re dont les puces investissent leurs ressources (transistors) :

  * **CPU (Optimis√© pour la Latence) :** Con√ßu pour que *chaque instruction* se termine le plus vite possible. Il dispose de peu de c≈ìurs, mais chacun est puissant, dot√© de larges caches et d'unit√©s de pr√©diction complexes pour g√©rer la logique s√©quentielle.
  * **GPU (Optimis√© pour le D√©bit) :** Sacrifie la complexit√© individuelle pour maximiser le nombre d'unit√©s de calcul (ALU). Il ne cherche pas √† aller vite pour *une* t√¢che, mais √† en ex√©cuter des milliers simultan√©ment.

> **üí° L'Analogie : Le Professeur et la Classe**
>
>   * **Le CPU est un Professeur de Math√©matiques √©m√©rite (ex: Einstein).** Il est brillant et rapide. Il peut r√©soudre des int√©grales complexes en un clin d'≈ìil. Mais s'il doit corriger 10 000 copies d'addition simple, il devra les faire l'une apr√®s l'autre. Cela prendra des heures.
>   * **Le GPU est une classe de 1 000 √©l√®ves de primaire.** Individuellement, ils sont lents et ne savent faire que des op√©rations simples. Mais si vous distribuez les 10 000 copies, ils peuvent en corriger 1 000 √† la fois. Le travail est fini en quelques secondes.

### 1.2 Traduction en CUDA

En CUDA, on distingue le code qui tourne sur le CPU (**Host**) de celui qui tourne sur le GPU (**Device**).

```cpp
// Code CPU (Le Professeur)
void main() {
    // Pr√©pare les donn√©es et lance l'ordre √† la classe
    monKernel<<<...>>>(...); 
}

// Code GPU (Les √âl√®ves)
// Le mot-cl√© "__global__" indique que cette fonction est ex√©cut√©e sur le GPU
__global__ void monKernel(float* data) {
    // Instruction simple ex√©cut√©e par des milliers de threads
}
```



## 2\. Architecture Mat√©rielle et Mod√®le d'Ex√©cution

Pour exploiter toute la puissance du GPU, CUDA d√©compose l'ex√©cution en une hi√©rarchie stricte. Une mauvaise gestion de cette structure peut fortement impacter les performances.

### 2.1 Hi√©rarchie Logique : Grille, Blocs et Threads

CUDA organise les "ouvriers" en trois niveaux :

1.  **La Grille (Grid) :** L'ensemble du probl√®me √† r√©soudre (ex: une image enti√®re).
2.  **Le Bloc (Thread Block) :** Un sous-groupe de la grille. Les threads d'un m√™me bloc peuvent communiquer via une m√©moire partag√©e rapide.
3.  **Le Thread (Fil d'ex√©cution) :** L'unit√© fondamentale qui traite un seul point de donn√©e (ex: un pixel).

> **üí° L'Analogie : L'Organisation de l'√âcole**
>
>   * **La Grille** est l'√©cole enti√®re mobilis√©e pour un examen.
>   * **Le Bloc** est une salle de classe sp√©cifique. Les √©l√®ves d'une m√™me salle peuvent se parler (m√©moire partag√©e), mais ne peuvent pas copier sur les √©l√®ves de la salle voisine.
>   * **Le Thread** est un √©l√®ve unique assis √† son bureau.

### 2.2 Traduction en CUDA : Le Calcul de Coordonn√©es

Chaque thread doit savoir "qui il est" pour savoir "quelle donn√©e traiter". Il calcule ses coordonn√©es uniques ($x, y$) √† partir de son index dans le bloc et de la position du bloc dans la grille.

```cpp
__global__ void imageKernel(int* image, int width) {
    // Qui suis-je ? (Calcul de l'index global)
    // blockIdx.x : Num√©ro de ma salle de classe
    // blockDim.x : Nombre d'√©l√®ves par salle
    // threadIdx.x : Mon num√©ro de place dans la salle
    
    int x = blockIdx.x * blockDim.x + threadIdx.x;
    int y = blockIdx.y * blockDim.y + threadIdx.y;

    // Je travaille uniquement si je suis dans les limites de l'image
    if (x < width && y < height) {
        // Traitement de MON pixel unique
    }
}
```

### 2.3 L'Unit√© R√©elle : Le Warp et la Divergence

Physiquement, le GPU n'ex√©cute pas les threads un par un, mais par groupes de 32 appel√©s **Warps**. Ils suivent le mod√®le SIMT (*Single Instruction, Multiple Threads*) : ils doivent tous ex√©cuter la m√™me instruction au m√™me moment.

**Le Pi√®ge : La Divergence**
Si vous mettez un `if-else` dans votre code, et que la moiti√© du Warp va dans le `if` et l'autre dans le `else`, le GPU doit ex√©cuter les deux branches s√©quentiellement, divisant la performance par deux.

> **üí° L'Analogie : La Dict√©e**
> Le professeur (l'unit√© de contr√¥le) dicte √† une rang√©e d'√©l√®ves (un Warp).
>
>   * *Cas id√©al :* "√âcrivez tous le mot 'Chat'". Tous √©crivent en m√™me temps.
>   * *Divergence :* "Si vous avez un stylo bleu, √©crivez 'Chat', sinon √©crivez 'Chien'". Le prof doit d'abord faire √©crire ceux au stylo bleu (les autres attendent), puis ceux au stylo rouge. On perd du temps.

-----

## 3\. Gestion de la M√©moire : Le Nerf de la Guerre

Dans une application GPU, la puissance de calcul brute est rarement le facteur limitant. Le v√©ritable goulot d'√©tranglement est la gestion de la m√©moire. Une application mal optimis√©e peut passer l'essentiel de son temps √† attendre l'arriv√©e des donn√©es.

### 3.1 Le Goulot d'√âtranglement : le Bus PCIe

L'architecture repose sur la s√©paration physique de deux espaces m√©moires (Host RAM et Device VRAM), connect√©s par le bus PCI Express (PCIe). La bande passante du bus PCIe est consid√©rablement plus lente (ex: 16-32 Go/s pour une liaison x16) que la bande passante interne de la m√©moire du GPU (ex: 900 Go/s pour une architecture Volta). Cette disparit√© cr√©e le goulot d'√©tranglement PCIe qui doit √™tre contourn√©.

La strat√©gie d'optimisation fondamentale consiste √† minimiser les op√©rations de Transfert Host $\leftrightarrow$ Device : il est imp√©ratif d'envoyer la totalit√© des donn√©es d'entr√©e au d√©but, d'ex√©cuter le calcul intensif sur le Device, puis de ne rapatrier que le r√©sultat final 4

> **üí° L'Analogie : La Biblioth√®que et la Salle de Classe**
>* **La RAM CPU (Host)** est la Biblioth√®que Universitaire (Source).
>* **La VRAM GPU (Device)** est la Salle d'Examen (Travail).
>* **Le Bus PCIe** est la Camionnette de Livraison.
>
>Le co√ªt majeur de l'op√©ration est li√© √† la latence de chaque transfert (le temps d'attente pour charger et d√©charger la camionnette). Pour maximiser l'efficacit√© (d√©bit), il ne faut jamais envoyer une camionnette pour un seul livre (petit transfert). Il faut consolider les besoins en remplissant la camionnette au maximum de sa capacit√© avec toutes les donn√©es requises, et n'effectuer qu'un seul aller-retour entre l'H√¥te et le Device.

### 3.2 Traduction en CUDA : Allocation et Transfert

La gestion m√©moire ressemble au C standard (`malloc`, `memcpy`) mais avec le pr√©fixe `cuda`.

```cpp
void gestionMemoire(int imageSize) {
    unsigned char *h_image, *d_image;

    // 1. Allocation CPU (Host)
    h_image = (unsigned char*)malloc(imageSize);

    // 2. Allocation GPU (Device) - On pr√©pare le tableau noir
    cudaMalloc((void**)&d_image, imageSize);

    // 3. Transfert CPU -> GPU (La camionnette part)
    cudaMemcpy(d_image, h_image, imageSize, cudaMemcpyHostToDevice);

    // ... Ex√©cution du Kernel sur d_image ...

    // 4. R√©cup√©ration GPU -> CPU (La camionnette revient)
    cudaMemcpy(h_image, d_image, imageSize, cudaMemcpyDeviceToHost);
    
    cudaFree(d_image);
}
```


## 4\. Application : Filtre S√©pia

Pour prouver la pertinence du GPU, nous appliquons un filtre S√©pia sur une image 4K (8 millions de pixels). C'est un probl√®me qui se pr√™te id√©alement au parall√©lisme massif.

### 4.1 Le Kernel Complet

Voici le c≈ìur du programme. Notez l'absence de boucle `for` : la boucle est remplac√©e par la grille de threads.

```cpp
__global__ void sepiaKernel(unsigned char* image, int width, int height) {
    // Calcul des coordonn√©es (L'√©l√®ve trouve sa place)
    int x = blockIdx.x * blockDim.x + threadIdx.x;
    int y = blockIdx.y * blockDim.y + threadIdx.y;

    if (x < width && y < height) {
        // Index lin√©aris√© pour acc√©der √† la m√©moire 1D
        int tid = (y * width + x) * 3; // 3 canaux (RGB)

        // Lecture des couleurs (L'√©l√®ve lit sa donn√©e)
        float r = image[tid];
        float g = image[tid+1];
        float b = image[tid+2];

        // Calcul S√©pia (L'√©l√®ve fait son calcul)
        float new_r = (r * 0.393) + (g * 0.769) + (b * 0.189);
        float new_g = (r * 0.349) + (g * 0.686) + (b * 0.168);
        float new_b = (r * 0.272) + (g * 0.534) + (b * 0.131);

        // √âcriture (L'√©l√®ve note le r√©sultat)
        image[tid] = (unsigned char)min(255.0f, new_r);
        image[tid+1] = (unsigned char)min(255.0f, new_g);
        image[tid+2] = (unsigned char)min(255.0f, new_b);
    }
}
```

### 4.2 Lancement depuis le CPU

Comment organiser notre arm√©e de threads ? Pour une image, on utilise g√©n√©ralement des blocs carr√©s de 16x16 threads (256 threads par bloc).

```cpp
int main() {
    // ... Allocation et Transferts (voir section 3.2) ...

    // D√©finition de la taille de l'√©quipe (Bloc)
    dim3 threadsPerBlock(16, 16); 

    // Calcul du nombre d'√©quipes n√©cessaires (Grille)
    // On divise la taille de l'image par 16, en arrondissant au sup√©rieur
    dim3 numBlocks((width + 15) / 16, (height + 15) / 16);

    // Lancement de l'assaut
    sepiaKernel<<<numBlocks, threadsPerBlock>>>(d_image, width, height);
    
    // Attente de la fin
    cudaDeviceSynchronize();
}
```

### 4.3\. Analyse de Performance

Est-ce que tout cet effort de programmation en vaut la peine ? Voici une comparaison typique pour le traitement d'une image haute d√©finition.

| M√©trique | CPU (Intel Core i7) | GPU (NVIDIA Tesla T4) | Gain |
| :--- | :--- | :--- | :--- |
| **M√©thode** | Boucle s√©quentielle | 8 millions de threads parall√®les | - |
| **Temps de calcul** | \~250 ms | \~3 ms | **x80** |
| **Philosophie** | Une Ferrari faisant 8 millions d'allers-retours | Un train de marchandises transportant tout d'un coup | - |

### 4.4\. Conclusion
Le GPU n'est pas "plus rapide" au sens o√π il court plus vite (fr√©quence en MHz souvent inf√©rieure au CPU). Il est plus performant car il est **plus large**. Pour des t√¢ches massives comme le traitement d'image ou l'IA, le paradigme CUDA permet de transformer un probl√®me temporel (attendre la fin de la boucle) en un probl√®me spatial (occuper toute la surface de la puce).
