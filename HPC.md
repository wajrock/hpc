# Introduction au Calcul Haute Performance sur GPU

## 1\. Introduction : Du Calcul S√©quentiel au Parall√©lisme 

Pendant des d√©cennies, l'informatique a repos√© sur un seul ma√Ætre √† bord : le **CPU** (Processeur Central). Imaginez-le comme un gestionnaire ultra-rapide, capable de passer d'un email √† un fichier Excel puis √† une page web en un √©clair. Sa force, c'est la **logique s√©quentielle** : il traite les probl√®mes complexes les uns apr√®s les autres.

Cependant, l'arriv√©e des jeux vid√©o 3D, du traitement d'image haute d√©finition et plus r√©cemment de l'Intelligence Artificielle (*Deep Learning*) a chang√© la donne. Ici, il ne s'agit plus de r√©soudre une √©quation complexe, mais de traiter **des millions de petites donn√©es identiques simultan√©ment** (par exemple : changer la couleur de 8 millions de pixels sur un √©cran 4K).

Face √† cette charge, le CPU s'essouffle. Ce cours vous introduit au **calcul GPU** (Graphics Processing Unit). C'est une approche qui abandonne la "vitesse pure sur une t√¢che" au profit du **parall√©lisme massif** : faire moins vite individuellement, mais faire tout en m√™me temps.


### 1.1 Comparaison Architecturale : Latence versus D√©bit

La diff√©rence fondamentale entre CPU et GPU r√©side dans la mani√®re dont les puces investissent leurs ressources (transistors) :

  * **CPU (Optimis√© pour la Latence) :** Con√ßu pour que *chaque instruction* se termine le plus vite possible. Il dispose de peu de c≈ìurs, mais chacun est puissant, dot√© de larges caches et d'unit√©s de pr√©diction complexes pour g√©rer la logique s√©quentielle.

  * **GPU (Optimis√© pour le D√©bit) :** Sacrifie la complexit√© individuelle pour maximiser le nombre d'unit√©s de calcul (ALU). Il ne cherche pas √† aller vite pour *une* t√¢che, mais √† en ex√©cuter des milliers simultan√©ment.

> **üí° L'Analogie : Le Professeur et la Classe**
>
>   * **Le CPU est un Professeur de Math√©matiques √©m√©rite (ex: Einstein).** Il est brillant et rapide. Il peut r√©soudre des int√©grales complexes en un clin d'≈ìil. Mais s'il doit corriger 10 000 copies d'addition simple, il devra les faire l'une apr√®s l'autre. Cela prendra des heures.
>   * **Le GPU est une classe de 1 000 √©l√®ves de primaire.** Individuellement, ils sont lents et ne savent faire que des op√©rations simples. Mais si vous distribuez les 10 000 copies, ils peuvent en corriger 1 000 √† la fois. Le travail est fini en quelques secondes.

### 1.2 Traduction en CUDA

En programmation CUDA, nous √©crivons tout dans le m√™me fichier (extension `.cu`), mais il faut comprendre que deux mondes physiquement s√©par√©s cohabitent :

1.  **Le Host (H√¥te) :** C'est votre **CPU**. Il joue le r√¥le de **Chef d'Orchestre**. Il ne fait pas le calcul intensif lui-m√™me, mais il g√®re la logistique : il pr√©pare les donn√©es, les envoie au GPU et donne le signal de d√©part.
2.  **Le Device (P√©riph√©rique) :** C'est votre **GPU**. C'est l'**Usine**. Il attend les ordres et les donn√©es pour lancer ses milliers d'ouvriers.

> **üí° L'Analogie : Les Consignes vs L'Exercice**
>
> * **Le Code Host (CPU)** correspond aux **consignes orales** du professeur : *"Prenez une feuille, recopiez l'exercice au tableau, vous avez 1 heure."*
> * **Le Code Device (GPU)** correspond √† **l'√©nonc√© de l'exercice** lui-m√™me : *"Calculez la racine carr√©e de x."* Chaque √©l√®ve (Thread) va appliquer cet √©nonc√© √† sa propre feuille.

Pour distinguer ces deux mondes dans le code, CUDA utilise un mot-cl√© sp√©cial : `__global__`.

```cpp
// Code CPU (Le Professeur)
void main() {
    // Pr√©pare les donn√©es et lance l'ordre √† la classe
    monKernel<<<...>>>(...); 
}

// Code GPU (Les √âl√®ves)
// Le mot-cl√© "__global__" indique que cette fonction est ex√©cut√©e sur le GPU
__global__ void monKernel(float* data) {
    // Instruction simple ex√©cut√©e par des milliers de threads
}
```

## 2\. Architecture Mat√©rielle et Mod√®le d'Ex√©cution

Pour exploiter toute la puissance du GPU, CUDA d√©compose l'ex√©cution en une hi√©rarchie stricte. Une mauvaise gestion de cette structure peut fortement impacter les performances.

### 2.1 Hi√©rarchie Logique : Grille, Blocs et Threads

CUDA organise les "ouvriers" en trois niveaux :

1.  **La Grille (Grid) :** L'ensemble du probl√®me √† r√©soudre (ex: une image enti√®re).
2.  **Le Bloc (Thread Block) :** Un sous-groupe de la grille. Les threads d'un m√™me bloc peuvent communiquer via une m√©moire partag√©e rapide.
3.  **Le Thread (Fil d'ex√©cution) :** L'unit√© fondamentale qui traite un seul point de donn√©e (ex: un pixel).

> **üí° L'Analogie : L'Organisation de l'√âcole**
>
>   * **La Grille** est l'√©cole enti√®re mobilis√©e pour un examen.
>   * **Le Bloc** est une salle de classe sp√©cifique. Les √©l√®ves d'une m√™me salle peuvent se parler (m√©moire partag√©e), mais ne peuvent pas copier sur les √©l√®ves de la salle voisine.
>   * **Le Thread** est un √©l√®ve unique assis √† son bureau.

### 2.2 Traduction en CUDA : Le Calcul de Coordonn√©es

Chaque thread doit savoir "qui il est" pour savoir "quelle donn√©e traiter". Il calcule ses coordonn√©es uniques ($x, y$) √† partir de son index dans le bloc et de la position du bloc dans la grille.

```cpp
__global__ void imageKernel(int* image, int width) {
    // Qui suis-je ? (Calcul de l'index global)
    // blockIdx.x : Num√©ro de ma salle de classe
    // blockDim.x : Nombre d'√©l√®ves par salle
    // threadIdx.x : Mon num√©ro de place dans la salle
    
    int x = blockIdx.x * blockDim.x + threadIdx.x;
    int y = blockIdx.y * blockDim.y + threadIdx.y;

    // Je travaille uniquement si je suis dans les limites de l'image
    if (x < width && y < height) {
        // Traitement de MON pixel unique
    }
}
```

### 2.3 L'Unit√© R√©elle : Le Warp et la Divergence

Physiquement, le GPU n'ex√©cute pas les threads un par un, mais par groupes de 32 appel√©s **Warps**. Ils suivent le mod√®le SIMT (*Single Instruction, Multiple Threads*) : ils doivent tous ex√©cuter la m√™me instruction au m√™me moment.

**Le Pi√®ge : La Divergence**
Si vous mettez un `if-else` dans votre code, et que la moiti√© du Warp va dans le `if` et l'autre dans le `else`, le GPU doit ex√©cuter les deux branches s√©quentiellement, divisant la performance par deux.

> **üí° L'Analogie : La Dict√©e**
> Le professeur (l'unit√© de contr√¥le) dicte √† une rang√©e d'√©l√®ves (un Warp).
>
>   * *Cas id√©al :* "√âcrivez tous le mot 'Chat'". Tous √©crivent en m√™me temps.
>   * *Divergence :* "Si vous avez un stylo bleu, √©crivez 'Chat', sinon √©crivez 'Chien'". Le prof doit d'abord faire √©crire ceux au stylo bleu (les autres attendent), puis ceux au stylo rouge. On perd du temps.

-----

## 3\. Gestion de la M√©moire : Le Nerf de la Guerre

Dans une application GPU, la puissance de calcul brute est rarement le facteur limitant. Le v√©ritable goulot d'√©tranglement est la gestion de la m√©moire. Une application mal optimis√©e peut passer l'essentiel de son temps √† attendre l'arriv√©e des donn√©es.

### 3.1 Le Goulot d'√âtranglement : le Bus PCIe

L'architecture repose sur la s√©paration physique de deux espaces m√©moires (Host RAM et Device VRAM), connect√©s par le bus PCI Express (PCIe). La bande passante du bus PCIe est consid√©rablement plus lente (ex: 16-32 Go/s pour une liaison x16) que la bande passante interne de la m√©moire du GPU (ex: 900 Go/s pour une architecture Volta). Cette disparit√© cr√©e le goulot d'√©tranglement PCIe qui doit √™tre contourn√©.

La strat√©gie d'optimisation fondamentale consiste √† minimiser les op√©rations de Transfert Host $\leftrightarrow$ Device : il est imp√©ratif d'envoyer la totalit√© des donn√©es d'entr√©e au d√©but, d'ex√©cuter le calcul intensif sur le Device, puis de ne rapatrier que le r√©sultat final 4

> **üí° L'Analogie : La Biblioth√®que et la Salle de Classe**
>* **La RAM CPU (Host)** est la Biblioth√®que Universitaire (Source).
>* **La VRAM GPU (Device)** est la Salle d'Examen (Travail).
>* **Le Bus PCIe** est la Camionnette de Livraison.
>
>Le co√ªt majeur de l'op√©ration est li√© √† la latence de chaque transfert (le temps d'attente pour charger et d√©charger la camionnette). Pour maximiser l'efficacit√© (d√©bit), il ne faut jamais envoyer une camionnette pour un seul livre (petit transfert). Il faut consolider les besoins en remplissant la camionnette au maximum de sa capacit√© avec toutes les donn√©es requises, et n'effectuer qu'un seul aller-retour entre l'H√¥te et le Device.

### 3.2 Traduction en CUDA : Allocation et Transfert

La gestion m√©moire ressemble au C standard (`malloc`, `memcpy`) mais avec le pr√©fixe `cuda`.

```cpp
void gestionMemoire(int imageSize) {
    unsigned char *h_image, *d_image;

    // 1. Allocation CPU (Host)
    h_image = (unsigned char*)malloc(imageSize);

    // 2. Allocation GPU (Device) - On pr√©pare le tableau noir
    cudaMalloc((void**)&d_image, imageSize);

    // 3. Transfert CPU -> GPU (La camionnette part)
    cudaMemcpy(d_image, h_image, imageSize, cudaMemcpyHostToDevice);

    // ... Ex√©cution du Kernel sur d_image ...

    // 4. R√©cup√©ration GPU -> CPU (La camionnette revient)
    cudaMemcpy(h_image, d_image, imageSize, cudaMemcpyDeviceToHost);
    
    cudaFree(d_image);
}
```


## 4. Application Concr√®te : Le Filtre S√©pia

Pour d√©montrer la sup√©riorit√© du GPU sur des t√¢ches massives, nous allons traiter une image 4K ($3840 \times 2160$ pixels). Cela repr√©sente **8,3 millions de pixels**.

Le but est d'appliquer un effet "S√©pia" (vieille photo). Pour l'ordinateur, cela signifie lire chaque pixel, m√©langer ses canaux Rouge-Vert-Bleu (RGB) selon une formule pr√©cise, et r√©√©crire le r√©sultat.


### 4.1. Le D√©fi : Comprendre la M√©moire 1D

 Nous voyons l'image comme une grille en 2D (lignes et colonnes), mais la m√©moire vid√©o (VRAM) stocke tout sur une seule ligne continue (1D), comme un immense ruban.

Pour qu'un thread (traitant le pixel $x, y$) trouve sa couleur sur ce ruban, il doit convertir ses coordonn√©es 2D en index 1D.

> **üí° L'Analogie : La Biblioth√®que**
>
> Imaginez une biblioth√®que avec 10 √©tag√®res ($y$) de 100 livres ($x$) chacune. Si vous voulez le 5√®me livre de la 3√®me √©tag√®re, combien de livres y a-t-il avant lui ?
>
> * Vous devez passer les 2 √©tag√®res compl√®tes pr√©c√©dentes ($y \times \text{largeur}$).
> * Plus les 5 livres de l'√©tag√®re actuelle ($+ x$).
>
> **La Formule :**
> $$Index = (y \times Largeur) + x$$

### 4.2. Le Code : 8 Millions de Peintres

Voici le **Kernel** (le code ex√©cut√© par le GPU). Remarquez qu'il n'y a aucune boucle **for**. Ce code d√©crit la t√¢che d'un seul thread pour un seul pixel.

Deux concepts sont souvent difficiles √† saisir ici, expliquons-les avant de voir le code :

1. Pourquoi * 3 dans l'index ? Chaque pixel est compos√© de 3 valeurs : Rouge, Vert, Bleu. Si le thread s'occupe du pixel n¬∞10, il ne doit pas √©crire √† la case 10 de la m√©moire, mais √† la case 30 (car les 10 pixels pr√©c√©dents occupent chacun 3 places).

2. Pourquoi min(255, ...) ? Le filtre S√©pia a tendance √† √©claircir l'image. Si le calcul donne "300", cela d√©passe la capacit√© d'un octet (max 255). Sans cette s√©curit√©, la valeur "d√©borderait" (300 devient 44) et cr√©erait des points noirs aberrants sur l'image.

```cpp
__global__ void sepiaKernel(unsigned char* image, int width, int height) {
    // --- √âTAPE 1 : IDENTIFICATION ---
    // Chaque "peintre" (thread) calcule sa position unique sur la toile
    int x = blockIdx.x * blockDim.x + threadIdx.x;
    int y = blockIdx.y * blockDim.y + threadIdx.y;

    // S√âCURIT√â : On v√©rifie qu'on ne peint pas hors du cadre
    if (x < width && y < height) {

        // --- √âTAPE 2 : LOCALISATION M√âMOIRE ---
        // On convertit la position (x,y) en adresse m√©moire lin√©aire.
        // On multiplie par 3 car chaque pixel contient 3 valeurs (R, G, B).
        int tid = (y * width + x) * 3;

        // --- √âTAPE 3 : LECTURE ---
        // On utilise des 'float' pour ne pas perdre de pr√©cision dans les calculs
        float r = image[tid];     // Rouge
        float g = image[tid+1];   // Vert
        float b = image[tid+2];   // Bleu

        // --- √âTAPE 4 : M√âLANGE (Formule S√©pia) ---
        // L'oeil humain est plus sensible au vert, d'o√π les coefficients diff√©rents.
        float new_r = (r * 0.393f) + (g * 0.769f) + (b * 0.189f);
        float new_g = (r * 0.349f) + (g * 0.686f) + (b * 0.168f);
        float new_b = (r * 0.272f) + (g * 0.534f) + (b * 0.131f);

        // --- √âTAPE 5 : √âCRITURE ---
        // On borne les valeurs √† 255 (min) pour √©viter les bugs d'affichage
        image[tid]   = (unsigned char)min(255.0f, new_r);
        image[tid+1] = (unsigned char)min(255.0f, new_g);
        image[tid+2] = (unsigned char)min(255.0f, new_b);
    }
}
```

### 4.3. Le Lancement (Host)

C'est ici que le CPU (le Chef) organise les √©quipes et lance le travail. Le d√©fi principal est de calculer combien de blocs (√©quipes) sont n√©cessaires pour couvrir toute l'image.

Pour comprendre le calcul, **visualisez une sortie scolaire g√©ante** :

> **L'Analogie : La Flotte de Bus**
>
> Imaginez que vous devez transporter tous les √©l√®ves de l'√©cole (vos pixels) vers le lieu de l'examen.
>
> * Vous disposez d'une flotte de **bus scolaires** (vos Blocs).
> * Chaque bus a exactement **16 places** (la dimension `threadsPerBlock`).
>
> **L'exemple pratique :** Si vous avez **100 √©l√®ves** √† transporter :
> 1.  Si vous faites une division simple : $100 / 16 = 6.25$.
> 2.  Si vous commandez **6 bus**, vous transportez 96 √©l√®ves et vous en laissez **4 sur le trottoir**.
> 3.  Il est donc imp√©ratif de commander **7 bus**, m√™me si le dernier part avec des si√®ges vides.

En informatique, cette "commande de bus suppl√©mentaire" se traduit par une formule d'arrondi au sup√©rieur :



```cpp
void main() {
    // ... (Allocation m√©moire et copie des donn√©es faites pr√©c√©demment) ...

    // 1. D√©finition de la taille d'un bus (Bloc)
    // 16x16 = 256 threads. C'est un standard efficace sur NVIDIA.
    dim3 threadsPerBlock(16, 16); 

    // 2. Commande du nombre de bus (Grille)
    // On utilise la formule d'arrondi pour couvrir toute l'image
    dim3 numBlocks((width + 15) / 16, (height + 15) / 16);

    // 3. LE D√âPART (Lancement du Kernel)
    // La syntaxe <<< >>> est sp√©cifique √† CUDA. C'est le "coup de pistolet" du d√©part.
    // Le CPU envoie l'ordre et continue sa vie sans attendre (asynchrone).
    sepiaKernel<<<numBlocks, threadsPerBlock>>>(d_image, width, height);

    // 4. Attente (Synchronisation)
    // Le CPU attend que le GPU ait fini avant de r√©cup√©rer les r√©sultats.
    cudaDeviceSynchronize();
}

```

### 4.4. Analyse de Performance

Est-ce que tout cet effort de programmation en vaut la peine ? Voici une comparaison typique pour le traitement d'une image haute d√©finition.

| M√©trique | CPU (Intel Core i7) | GPU (NVIDIA Tesla T4) | Gain |
| :--- | :--- | :--- | :--- |
| **M√©thode** | Boucle s√©quentielle | 8 millions de threads parall√®les | - |
| **Temps de calcul** | \~250 ms | \~3 ms | **x80** |
| **Philosophie** | Une Ferrari faisant 8 millions d'allers-retours | Un train de marchandises transportant tout d'un coup | - |

On peut remarquer que Les r√©sultats sont sans appel : le GPU est infiniment plus efficace pour cette t√¢che. Mais pourquoi ?

**L'Explication Technique : S√©quentiel vs Parall√®le**

* **Le CPU** ex√©cute une boucle `for` g√©ante. Il doit traiter le pixel 1, *puis* le pixel 2, *puis* le 3... jusqu'au 8 300 000√®me. M√™me s'il va tr√®s vite pour chaque pixel, l'addition des temps cr√©e une latence √©lev√©e.

* **Le GPU** supprime la notion de temps pour la remplacer par de l'espace. Il n'attend pas que le pixel 1 soit fini pour commencer le 2. Il lance **tous les calculs en m√™me temps** sur ses milliers d'unit√©s de calcul.

> **üèéÔ∏è L'Analogie : La Livraison de Pizzas**
>
> Imaginez que vous devez livrer 8 millions de pizzas.
>
> * **Le CPU est une Ferrari.** Elle roule √† 300 km/h. Mais elle ne peut transporter qu'une seule pizza √† la fois. Elle doit faire 8 millions d'allers-retours.
> * **Le GPU est une arm√©e de v√©los (ou un train de marchandises).** Ils roulent lentement (20 km/h). Mais ils partent tous en m√™me temps.
> **R√©sultat :** La premi√®re pizza livr√©e par la Ferrari arrive tr√®s vite (faible latence), mais pour livrer l'ensemble, l'arm√©e de v√©los finit des heures avant (haut d√©bit).

### 4.5. Conclusion 

En r√©sum√©, il ne faut pas retenir que le GPU est "plus rapide" que le CPU (sa fr√©quence en MHz est souvent inf√©rieure), mais qu'il est **massivement parall√®le**.

Ce cours vous a invit√© √† un changement de philosophie fondamental : nous sommes pass√©s d'une architecture optimis√©e pour la **latence** (ex√©cuter une t√¢che le plus vite possible) √† une architecture d√©di√©e au **d√©bit** (ex√©cuter des milliers de t√¢ches simultan√©ment).

Ma√Ætriser CUDA, ce n'est pas seulement apprendre une nouvelle syntaxe. C'est comprendre comment transformer un probl√®me temporel (attendre la fin d'une boucle) en un probl√®me spatial (occuper toute la surface de la puce avec des milliers de threads). C'est cette capacit√© √† "diviser pour r√©gner" √† grande √©chelle qui rend aujourd'hui possibles les avanc√©es majeures en *Deep Learning* et en simulation scientifique.
